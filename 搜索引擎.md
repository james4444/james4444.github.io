# 数据结构与算法

## 搜索引擎

搜索引擎分为四个部分

* 搜集

利用爬虫爬取网页。把互联网看成一个有向图，把每个页面看成一个顶点，引用的链接看作指向另一个顶点的边，我们利用图的遍历，来遍历整个互联网中的网页。

搜索引擎采用的是广度优先的策略，我们先找一些权重比较高的链接，作为种子网页链接，放入到队列中，按照广度优先的策略，不停的从队列中取出链接，爬取对应的网页。

关键技术细节
1. 待爬取网页链接文件links.bin。链接放不下，可以存储到文件中。支持断点续爬

2. 网页判重，可以使用布隆过滤器 。为了防止断电导致布隆锅过滤器数据消失，我们可以将布隆过滤器存储到磁盘上，bloom_filter.bin，

3. 原始网页存储，合并存储，doc_raw.bin

4. 网页链接及其编号对应的文件 doc_id.bin。在存储网页的同时，我们将网页和编号的对应关系，存储在这个文件中。

* 分析

内容抽取、分词、构建倒排索引。网页爬取下来之后，我们需要对网页进行离线分析，分析主要包含两个步骤：
1. 抽取网页文本信息，去掉html标签
2. 分词并创建临时索引，基于字典或基于规则的分词。基于字典库的分词，我们可以利用trie树，取最长匹配。每个网页分析完成后，得到一组单词列表。我们把单词与网页之间的关系，写入到临时索引文件 temp_index.bin中。在临时文件temp_index.bin中，我们存储的不是单词本身，而是单词编号，主要是为了减少存储空间。当所有的网页处理好之后，我们在将单词编号和网页变好的关系固化到磁盘中，并命名为term_id.bin

* 索引

通过分析阶段的临时索引，构建倒排索引。倒排索引记录了每个单词以及包含它的网页列表，考虑到临时索引文件一般很大，无法一次性加载到内存中，搜索引擎一般采用**多路归并排序**来实现。除了倒排文件外，我们还需要一个文件，来记录每个单词在倒排索引中的偏移位置，我们把这个文件命名为term_offset.bin。经过索引阶段的处理，我们得到了两个文件，倒排索引文件（index.bin）和单词在文件中的偏移位置文件（term_offset.bin）

* 查询

根据倒排索引，获得相关网页，计算网页排名，返回查询结果给用户。经过前三个阶段的处理，我们得到了四个重要文件
1. doc_id.bin 记录网页链接和编号的关系
2. term_id.bin 记录单词和单词编号的关系
3. index.bin 倒排索引文件，记录每个单词编号以及对应包含它的网页编号的列表
4. term_offset.bin 记录每个单词编号，在倒排索引文件中的偏移位置

这四个文件中，除了index.bin之外，其余都比较小。我们把其余三个文件都加载到内存中，构建成散列表

当用户搜索时，我们对用户的输入进行分词，得到k个单词。通过查询term_id.bin，可以得到单词对应的单词编号。通过单词编号查询 term_offset.bin，可以得到单词在index.bin中的位置，查询index.bin得到每个单词包含的网页链接，对网页链接进行计数，按照从多倒是倒叙分页返回




